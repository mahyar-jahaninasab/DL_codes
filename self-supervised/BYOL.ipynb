{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnKVArh_0qhm"
      },
      "source": [
        "# BYOL\n",
        "\n",
        "In this notebook we are going to implement [BYOL: Bootstrap Your Own Latent](https://arxiv.org/pdf/2006.07733.pdf) and compare the results of a classification task before and after pretraining the model with BYOL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E6nh_5Q2uIp"
      },
      "source": [
        "### Data Augmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7vzr4sTnriT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Callable, Tuple\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, Tensor\n",
        "from torchvision import transforms as T\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn: Callable, p: float):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        if random.random() < self.p:\n",
        "            x = self.fn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def default_augmentation(image_size: Tuple[int, int] = (224, 224)) -> nn.Module:\n",
        "    \"\"\"\n",
        "        1. resize images to 'image_size'\n",
        "        2. RandomApply color jitter\n",
        "        3. RandomApply grayscale\n",
        "        4. RandomApply horizon flip\n",
        "        5. RandomApply gaussian blur with kernel_size(3, 3), sigma=(1.5, 1.5)\n",
        "        6. RandomApply ResizedCrop to 'image_size'\n",
        "        7. Normalize\n",
        "        choosing hyperparameters that are not mentioned is up to you\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        # your code\n",
        "\n",
        "        T.Resize((image_size)),\n",
        "        RandomApply(T.ColorJitter(),0.3),\n",
        "        RandomApply(T.Grayscale(num_output_channels=3),0.3),#return 3 channel or 1?\n",
        "        RandomApply(T.RandomHorizontalFlip(p=1),0.3),#I used RandomHorizontalFlip with p=1 so it always happens then I used randomapply\n",
        "        RandomApply(T.GaussianBlur(kernel_size=(3, 3), sigma=(1.5, 1.5)),0.3),\n",
        "        RandomApply(T.RandomResizedCrop(image_size),0.3),\n",
        "        T.Normalize(\n",
        "            mean=torch.tensor([0.485, 0.456, 0.406]),\n",
        "            std=torch.tensor([0.229, 0.224, 0.225]),\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KouhIzoTTjYi"
      },
      "source": [
        "# Model\n",
        "We will use ResNet18 as our representation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoU-3B4FmtrZ"
      },
      "outputs": [],
      "source": [
        "def get_encoder_model():\n",
        "    resnet = torchvision.models.resnet18()\n",
        "    # remove last fully-connected layer\n",
        "    # your code\n",
        "    layers = list(resnet.children())[:-1]\n",
        "    return nn.Sequential(*layers)\n",
        "#get_encoder_model()(torch.randn(1,3,224,224)) #just to ensure it works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tf7Y2hrTjYk"
      },
      "source": [
        "### Loss Function\n",
        "We need to use NormalizedMSELoss as our loss function.\n",
        "$$NormalizedMSELoss(v_1, v_2) = \\Vert \\bar{v_1} - \\bar{v_2}\\Vert_2^2 = 2 - 2.\\frac{\\langle v_1, v_2 \\rangle}{\\Vert v_1\\Vert_2 \\Vert v_2\\Vert_2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StyBpng0TjYl"
      },
      "outputs": [],
      "source": [
        "class NormalizedMSELoss(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(NormalizedMSELoss,self).__init__()\n",
        "\n",
        "    def forward(self, view1: Tensor, view2: Tensor) -> Tensor:\n",
        "        # your code\n",
        "        norm_view1 = torch.sqrt(torch.sum(view1 ** 2, dim=1, keepdim=True))\n",
        "        norm_view2 = torch.sqrt(torch.sum(view2 ** 2, dim=1, keepdim=True))\n",
        "        dot_product = view1*view2\n",
        "\n",
        "        return 2 - 2*torch.sum((dot_product/(norm_view1*norm_view2)),dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVPcRP87TjYl"
      },
      "source": [
        "### MLP\n",
        "Here you will implement a simple MLP class with one hidden layer with BatchNorm and ReLU activation, and a linear output layer. This class will be used for both the projections and the prediction networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FG17n9NTjYm"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, projection_dim: int = 256, hidden_dim: int = 4096) -> None:\n",
        "        super(MLP,self).__init__()\n",
        "\n",
        "        # your code\n",
        "        self.ly1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, projection_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.flatter = nn.Flatten()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # your code\n",
        "        #print(x.shape)\n",
        "        x = self.flatter(x)\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.bn1(self.ly1(x)))\n",
        "        x = self.out(x)\n",
        "        #print('mlp',x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grCbASvITjYn"
      },
      "source": [
        "### Encoder + Projector Network\n",
        "This is the network structure that is shared between online and target networks. It consists of our encoder model, followed by a projection MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1p5ZutsTjYo"
      },
      "outputs": [],
      "source": [
        "class EncoderProjecter(nn.Module):\n",
        "    def __init__(self,\n",
        "                 encoder: nn.Module,\n",
        "                 hidden_dim: int = 4096,\n",
        "                 projection_out_dim: int = 256\n",
        "                 ) -> None:\n",
        "        super(EncoderProjecter, self).__init__()\n",
        "\n",
        "        # your code\n",
        "        self.encoder = encoder\n",
        "        self.projector = MLP(512)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # your code\n",
        "        x = self.encoder(x)\n",
        "        x = self.projector(x)\n",
        "        #print(x.shape)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-Eb_OvZTjYo"
      },
      "source": [
        "## BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V887pFcTTjYp"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model: nn.Module,\n",
        "                 hidden_dim: int = 4096,\n",
        "                 projection_out_dim: int = 256,\n",
        "                 target_decay: float = 0.99\n",
        "\n",
        "                ) -> None:\n",
        "        super(BYOL, self).__init__()\n",
        "\n",
        "        # your code\n",
        "        self.online_network = nn.Sequential(model,MLP(512))# encoder + projector\n",
        "        self.online_predictor = MLP(256)\n",
        "\n",
        "        self.target_network =  nn.Sequential(model,MLP(512))  # init with copy of parameters of online network\n",
        "        # set target_network's weights to be untrainable\n",
        "        #self.target_network.load_state_dict(copy.deepcopy(self.online_network.state_dict()))\n",
        "        for parameter_t,parameter_o in zip(self.target_network.parameters(),self.online_network.parameters()):\n",
        "          parameter_t.data = copy.deepcopy(parameter_o.data)\n",
        "          parameter_t.requires_grad = False\n",
        "\n",
        "        self.tau = target_decay\n",
        "\n",
        "        self.target_network.eval()\n",
        "        self.loss_function = NormalizedMSELoss()\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def soft_update_target_network(self) -> None:\n",
        "        # your code\n",
        "        for parameter_t,parameter_o in zip(self.target_network.parameters(),self.online_network.parameters()):\n",
        "          parameter_t.data = parameter_o.data*self.tau + parameter_t.data*(1. - self.tau) #This is the moving average\n",
        "\n",
        "\n",
        "    def forward(self, view) -> Tuple[Tensor]:\n",
        "        # return online projection and target projection of view\n",
        "        # your code\n",
        "        online_projection = self.online_network(view)\n",
        "        #online_prediction = self.online_predictor(online_projection)#This block asks just for online projection\n",
        "        #target projection\n",
        "        with torch.no_grad():\n",
        "          target_projection = self.target_network(view)\n",
        "\n",
        "        return online_projection,target_projection\n",
        "\n",
        "    def loss(self, view1, view2):\n",
        "        # compute loss once for (online_prediction1, target_projection2) and once for (online_prediction2, target_projection1).\n",
        "        # then return the mean.\n",
        "        # your code\n",
        "        online_projection_view1,target_projection_view1 = self.forward(view1)\n",
        "        #print('check',online_projection_view1.shape,target_projection_view1.shape)\n",
        "        #print(online_projection_view1.shape)\n",
        "        online_prediction_view1 = self.online_predictor(online_projection_view1)\n",
        "        loss = self.loss_function(online_prediction_view1,target_projection_view1)\n",
        "\n",
        "        online_projection_view2,target_projection_view2 = self.forward(view2)\n",
        "        online_prediction_view2 = self.online_predictor(online_projection_view2)\n",
        "        loss += self.loss_function(online_prediction_view2,target_projection_view2)\n",
        "\n",
        "\n",
        "        return torch.mean(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQzcP89G4L2u"
      },
      "source": [
        "# STL10 Datasets\n",
        "\n",
        "We need 3 separate datasets from STL10 for this experiment:\n",
        "1. `\"train\"` -- Contains only labeled training images. Used for supervised training.\n",
        "2. `\"train+unlabeled\"` -- Contains training images, plus a large number of unlabelled images.  Used for self-supervised learning with BYOL.\n",
        "3. `\"test\"` -- Labeled test images.  We use it both as a validation set, and for computing the final model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lTVx52t9Kjf",
        "outputId": "564322e5-6b0c-45fe-88b5-56ca372a4cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2640397119/2640397119 [01:26<00:00, 30380405.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/stl10_binary.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import STL10\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "TRAIN_DATASET = STL10(root=\"data\", split=\"train\", download=True, transform=ToTensor())\n",
        "TRAIN_UNLABELED_DATASET = STL10(root=\"data\", split=\"train+unlabeled\", download=True, transform=ToTensor())\n",
        "TEST_DATASET = STL10(root=\"data\", split=\"test\", download=True, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N39j6w-JTjYr"
      },
      "source": [
        "Create dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBRdB-BDTjYr"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(dataset=TRAIN_DATASET,batch_size=1024,shuffle=True)\n",
        "train_unlabeld_dataloader = DataLoader(dataset=TRAIN_UNLABELED_DATASET,batch_size=1024,num_workers=2,shuffle=True)\n",
        "test_dataloader = DataLoader(dataset=TEST_DATASET,batch_size=1024,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdlrDnwR45bm"
      },
      "source": [
        "# Supervised Training without BYOL\n",
        "\n",
        "First create a classifier model by simply adding a linear layer on top of the encoder model. Then train the model using the labeled training set. Performance should be pretty good already."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Km2XBrWHTjYr"
      },
      "outputs": [],
      "source": [
        "encoder = get_encoder_model()\n",
        "# your code\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "classifier = nn.Linear(512,10)\n",
        "model = nn.Sequential(encoder,torch.nn.Flatten() ,classifier).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60-FyD6VB9Jn",
        "outputId": "40e44330-5957-4069-b625-2193c6e49a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/150 [00:17<44:17, 17.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train loss = 17.9206, Test loss = 0.0027, Test accuracy = 0.1429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 21/150 [02:12<15:45,  7.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train loss = 6.1037, Test loss = 0.0013, Test accuracy = 0.4979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 41/150 [04:08<13:03,  7.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Train loss = 1.0280, Test loss = 0.0020, Test accuracy = 0.5643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 61/150 [06:06<10:48,  7.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60: Train loss = 0.1171, Test loss = 0.0029, Test accuracy = 0.5933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 81/150 [08:03<08:15,  7.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80: Train loss = 0.0041, Test loss = 0.0034, Test accuracy = 0.5897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 101/150 [10:00<05:51,  7.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Train loss = 0.0124, Test loss = 0.0035, Test accuracy = 0.5916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 121/150 [11:57<03:30,  7.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 120: Train loss = 0.1291, Test loss = 0.0030, Test accuracy = 0.5797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 141/150 [13:53<01:04,  7.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140: Train loss = 0.0004, Test loss = 0.0033, Test accuracy = 0.5951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [14:43<00:00,  5.89s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 150\n",
        "lr = 1e-2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    total_loss = 0.\n",
        "    for batch, target in train_dataloader:\n",
        "        batch = batch.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        eval_loss = 0.\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch, target in test_dataloader:\n",
        "                batch = batch.to(DEVICE)\n",
        "                target = target.to(DEVICE)\n",
        "\n",
        "                output = model(batch)\n",
        "                eval_loss += criterion(output, target).item()\n",
        "\n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        eval_loss /= len(test_dataloader.dataset)\n",
        "        accuracy = correct / len(test_dataloader.dataset)\n",
        "        print(f\"Epoch {epoch}: Train loss = {total_loss:.4f}, Test loss = {eval_loss:.4f}, Test accuracy = {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBTyX-f45Sgj"
      },
      "source": [
        "### Self-Supervised Training with BYOL\n",
        "\n",
        "Now perform the self-supervised training. This is the most computationally intensive part of the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILv3AXqBTjYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296303bd-da88-4d6f-97d3-dccc0735ee00"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "  5%|▌         | 1/20 [06:03<1:55:08, 363.60s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train loss = 138.6696, Test loss = 0.0032, Test accuracy = 0.5976\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 3/20 [18:11<1:43:04, 363.77s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train loss = 3.7918, Test loss = 0.0033, Test accuracy = 0.5975\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 5/20 [30:18<1:30:56, 363.77s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train loss = 0.2558, Test loss = 0.0033, Test accuracy = 0.5981\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 7/20 [42:25<1:18:44, 363.41s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train loss = 0.1170, Test loss = 0.0033, Test accuracy = 0.5975\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 9/20 [54:30<1:06:32, 362.97s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Train loss = 0.0697, Test loss = 0.0033, Test accuracy = 0.5954\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 11/20 [1:06:36<54:27, 363.01s/it]  "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train loss = 0.0617, Test loss = 0.0033, Test accuracy = 0.5969\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 65%|██████▌   | 13/20 [1:18:41<42:19, 362.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train loss = 0.0471, Test loss = 0.0032, Test accuracy = 0.5995\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▌  | 15/20 [1:30:46<30:13, 362.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train loss = 0.0537, Test loss = 0.0033, Test accuracy = 0.5960\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▌ | 17/20 [1:42:52<18:07, 362.66s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train loss = 0.0647, Test loss = 0.0033, Test accuracy = 0.5971\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▌| 19/20 [1:54:58<06:02, 362.86s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train loss = 0.0535, Test loss = 0.0033, Test accuracy = 0.5965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [2:01:01<00:00, 363.09s/it]\n"
          ]
        }
      ],
      "source": [
        "epochs=20\n",
        "lr = 1e-2\n",
        "\n",
        "model_ = BYOL(get_encoder_model()).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model_.parameters(),lr=lr)\n",
        "\n",
        "augmenter = default_augmentation()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  total_loss = 0.\n",
        "  for data, _ in  train_unlabeld_dataloader:\n",
        "\n",
        "    data = data.to(DEVICE)\n",
        "    view1 = augmenter(data)\n",
        "    view2 = augmenter(data)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss = model_.loss(view1,view1)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model_.soft_update_target_network()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  #model.eval()\n",
        "  eval_loss = 0.\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "      for batch, target in test_dataloader:\n",
        "          batch = batch.to(DEVICE)\n",
        "          target = target.to(DEVICE)\n",
        "\n",
        "          output = model(batch)\n",
        "          eval_loss += criterion(output, target).item()\n",
        "\n",
        "          pred = output.argmax(dim=1, keepdim=True)\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "  eval_loss /= len(test_dataloader.dataset)\n",
        "  accuracy = correct / len(test_dataloader.dataset)\n",
        "\n",
        "  # Print training and evaluation metrics\n",
        "  if epoch % 2 == 0:\n",
        "      print(f\"Epoch {epoch}: Train loss = {total_loss:.4f}, Test loss = {eval_loss:.4f}, Test accuracy = {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMmW9Tj5k-j"
      },
      "source": [
        "### Supervised Training Again\n",
        "\n",
        "Extract the encoder network's state dictionary from BYOL, and load it into our ResNet18 model before starting training.  Then run supervised training, and watch the accuracy improve from last time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70eRZ-Qb0mcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29738b4c-864b-4a32-ceca-94739a243aa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "byol_res= copy.deepcopy(list(model_.children())[2][:-1])\n",
        "byol_res_state = byol_res.state_dict()\n",
        "\n",
        "#There was 0. before each key name so we need to clean it before loading it\n",
        "byol_res_state_edited = {}\n",
        "for key in byol_res_state.keys():\n",
        "    byol_res_state_edited[key[2:]] = byol_res_state[key]\n",
        "\n",
        "encoder_= get_encoder_model()\n",
        "encoder_.load_state_dict(byol_res_state_edited)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqO_80gc4DT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "473420eb-50b3-468a-ba3e-3c4bfeabfe87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 1/150 [00:11<27:47, 11.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train loss = 18.3458, Test loss = 0.0025, Test accuracy = 0.1000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 21/150 [03:51<23:32, 10.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Train loss = 5.5304, Test loss = 0.0014, Test accuracy = 0.4998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 41/150 [07:33<20:13, 11.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40: Train loss = 0.6332, Test loss = 0.0023, Test accuracy = 0.5539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 61/150 [11:14<16:28, 11.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60: Train loss = 0.0309, Test loss = 0.0031, Test accuracy = 0.5697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 81/150 [14:56<12:44, 11.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80: Train loss = 0.0004, Test loss = 0.0031, Test accuracy = 0.5863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 101/150 [18:37<09:01, 11.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Train loss = 0.0002, Test loss = 0.0032, Test accuracy = 0.5885\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 121/150 [22:17<05:19, 11.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 120: Train loss = 0.0002, Test loss = 0.0032, Test accuracy = 0.5893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 141/150 [25:57<01:38, 10.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 140: Train loss = 0.0001, Test loss = 0.0033, Test accuracy = 0.5859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 150/150 [27:36<00:00, 11.04s/it]\n"
          ]
        }
      ],
      "source": [
        "classifier = nn.Linear(512,10)\n",
        "model_last = nn.Sequential(encoder_,torch.nn.Flatten() ,classifier).to(DEVICE)\n",
        "epochs=150\n",
        "lr = 1e-2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_last.parameters(),lr=lr)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    # Training loop\n",
        "    total_loss = 0.\n",
        "    for batch, target in train_dataloader:\n",
        "        batch = batch.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model_last(batch)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Evaluation loop\n",
        "    #model_last.eval()\n",
        "    eval_loss = 0.\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch, target in test_dataloader:\n",
        "            batch = batch.to(DEVICE)\n",
        "            target = target.to(DEVICE)\n",
        "\n",
        "            output = model_last(batch)\n",
        "            eval_loss += criterion(output, target).item()\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    eval_loss /= len(test_dataloader.dataset)\n",
        "    accuracy = correct / len(test_dataloader.dataset)\n",
        "\n",
        "    # Print training and evaluation metrics\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}: Train loss = {total_loss:.4f}, Test loss = {eval_loss:.4f}, Test accuracy = {accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}